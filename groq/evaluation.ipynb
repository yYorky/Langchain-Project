{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROUGE Scores\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap of n-grams between the reference and generated responses. \n",
    "- ROUGE-1, ROUGE-2, and ROUGE-L are commonly used metrics:\n",
    "\n",
    "1. ROUGE-1: Overlap of unigrams (single words).\n",
    "2. ROUGE-2: Overlap of bigrams (two-word sequences).\n",
    "3. ROUGE-L: Longest common subsequence.\n",
    "\n",
    "You can use the rouge-score library in Python to compute these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute BLEU Scores\n",
    "- BLEU (Bilingual Evaluation Understudy) measures the precision of n-grams in the generated responses with respect to the reference responses. \n",
    "- BLEU-1, BLEU-2, BLEU-3, and BLEU-4 are commonly used metrics, with BLEU-4 being the most popular for overall evaluation.\n",
    "- You can use the nltk library in Python to compute BLEU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas rouge-score nltk openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average ROUGE-1</th>\n",
       "      <th>Average ROUGE-2</th>\n",
       "      <th>Average ROUGE-L</th>\n",
       "      <th>Average BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3-8b-8192 (OpenAIEmbedding/session_state....</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>0.046611</td>\n",
       "      <td>0.108567</td>\n",
       "      <td>0.014828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3-8b-8192 (OllamaEmbedding/session_state....</td>\n",
       "      <td>0.103982</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.077843</td>\n",
       "      <td>0.006972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-8b-8192 (GoogleGenAIEmbedding/session_s...</td>\n",
       "      <td>0.213687</td>\n",
       "      <td>0.097963</td>\n",
       "      <td>0.178456</td>\n",
       "      <td>0.029943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Average ROUGE-1  \\\n",
       "0  llama3-8b-8192 (OpenAIEmbedding/session_state....         0.133607   \n",
       "1  llama3-8b-8192 (OllamaEmbedding/session_state....         0.103982   \n",
       "2  llama3-8b-8192 (GoogleGenAIEmbedding/session_s...         0.213687   \n",
       "\n",
       "   Average ROUGE-2  Average ROUGE-L  Average BLEU  \n",
       "0         0.046611         0.108567      0.014828  \n",
       "1         0.024111         0.077843      0.006972  \n",
       "2         0.097963         0.178456      0.029943  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'Ground Truth.xlsx'\n",
    "sheet_name = 'Q&A Walkthro'\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Extract the relevant columns\n",
    "reference_responses = data['Answer'].tolist()\n",
    "model_columns = [\n",
    "    'llama3-8b-8192 (OpenAIEmbedding/session_state.docs (all))',\n",
    "    'llama3-8b-8192 (OllamaEmbedding/session_state.docs (all))',\n",
    "    'llama3-8b-8192 (GoogleGenAIEmbedding/session_state.docs (all))'\n",
    "]\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize BLEU scorer\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# List to store aggregated scores\n",
    "aggregated_scores_list = []\n",
    "\n",
    "# Compute ROUGE and BLEU scores for each model\n",
    "for model in model_columns:\n",
    "    generated_responses = data[model].tolist()\n",
    "    \n",
    "    # Lists to store individual scores\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    # Compute ROUGE and BLEU scores\n",
    "    for ref, gen in zip(reference_responses, generated_responses):\n",
    "        # Compute ROUGE scores\n",
    "        rouge_scores = scorer.score(ref, gen)\n",
    "        rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "        \n",
    "        # Compute BLEU scores\n",
    "        ref_tokens = ref.split()\n",
    "        gen_tokens = gen.split()\n",
    "        bleu_score = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothie)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_rouge_1 = np.mean(rouge_1_scores)\n",
    "    avg_rouge_2 = np.mean(rouge_2_scores)\n",
    "    avg_rouge_l = np.mean(rouge_l_scores)\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    # Append the scores to the list\n",
    "    aggregated_scores_list.append({\n",
    "        'Model': model,\n",
    "        'Average ROUGE-1': avg_rouge_1,\n",
    "        'Average ROUGE-2': avg_rouge_2,\n",
    "        'Average ROUGE-L': avg_rouge_l,\n",
    "        'Average BLEU': avg_bleu\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "aggregated_scores = pd.DataFrame(aggregated_scores_list)\n",
    "\n",
    "# Display the aggregated scores DataFrame\n",
    "aggregated_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Scores\n",
    "\n",
    "**Interpretation of ROUGE Scores:**\n",
    "\n",
    "- ROUGE-1: Measures the overlap of unigrams (single words). Higher scores indicate better recall of individual words from the reference responses.\n",
    "- ROUGE-2: Measures the overlap of bigrams (two-word sequences). Higher scores indicate better capture of contextual information.\n",
    "- ROUGE-L: Measures the longest common subsequence. Higher scores indicate better overall structure and coherence.\n",
    "\n",
    "\n",
    "**Interpretation of BLEU Scores:**\n",
    "\n",
    "- BLEU scores range from 0 to 1, with higher scores indicating better precision of n-grams in the generated responses relative to the reference responses.\n",
    "- BLEU is often used with a smoothing function (as done here) to handle cases where there are no matches, especially for short sentences.\n",
    "\n",
    "\n",
    "**Benchmarking:**\n",
    "\n",
    "- Compare the aggregated scores to benchmarks from similar systems or previous versions of your chatbot.\n",
    "- Higher scores generally indicate better performance, but the specific thresholds for \"good\" scores can vary by application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
